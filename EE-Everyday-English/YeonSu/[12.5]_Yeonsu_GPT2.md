# GPT-2

### Draft

Recently, I read a paper that title is “Language Models are Unsupervised Multitask Learners”. 
It's actually a paper I read once before when I was studying transformers.
The paper is written about GPT-2.

There are three difference with GPT-1.  So i will review that three features for you.
First, GPT-2 is larger than GPT-1.
This is obvious, but the feature is special because that is biggest change in this paper.
Second, GPT-2 is trained by large and great dataset.
They built a large dataset by scraping posts on Reddit with at least three recommendations.
Third,  GPT-2 used ‘layer normalization’.
Layer normalization is a type of normalization that normalizes on a sample-by-sample unit, and is a way to complement for large gradients early in training in a transformer architecture.

I was also impressed with what this paper had to say about zero-shot pretrained learning.
Reading Transformers and revisiting the paper was a great experience as it gave me a better understanding of the background of the paper.

### Korean

최근에 "언어 모델은 비지도 다중 작업 학습자"라는 논문을 읽었습니다.
사실 전에 트랜스포머를 공부하기 전에 한 번 읽었던 논문입니다.
이 논문은 GPT-2에 대해 쓰여져 있습니다.

GPT-2와 GPT-1의 세 가지 차이점이 있습니다. 그래서 이 세 가지 특징을 리뷰하겠습니다.
첫째, GPT-2는 GPT-1보다 큽니다.
이것은 당연하지만, 이 특징은 이 논문에서 가장 큰 변화입니다.
둘째, GPT-2는 크고 훌륭한 데이터셋으로 훈련되었습니다.
Reddit에서 적어도 세 개의 추천을 받은 게시물을 스크래핑하여 큰 데이터셋을 구축했습니다.
셋째, GPT-2는 '레이어 정규화'를 사용했습니다.
레이어 정규화는 샘플 단위로 정규화하는 정규화 유형으로, 트랜스포머 아키텍처에서 초기 학습 단계에서 큰 기울기를 보완 위한 방법입니다.

GPT-2 논문에서는 zero-shot pretrained learning에 대해 시사한 점도 감명이 깊었습니다.
트랜스포머를 읽고 논문을 다시한번 더 본것은 이 논문의 배경을 더 잘 이해할 수 있어 좋은 경험이었습니다.
