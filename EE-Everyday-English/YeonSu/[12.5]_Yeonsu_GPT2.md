# GPT-2

### Content URL

https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf

### Draft

Recently, I read a paper that title is “Language Models are Unsupervised Multitask Learners”. 
It's actually a paper I read once before when I was studying transformers.
The paper is written about GPT-2.

There are three difference with GPT-1.  So i will review that three features for you.
First, GPT-2 is larger than GPT-1.
This is obvious, but the feature is special because that is biggest change in this paper.
Second, GPT-2 is trained by large and great dataset.
They built a large dataset by scraping posts on Reddit with at least three recommendations.
Third,  GPT-2 used ‘layer normalization’.
Layer normalization is a type of normalization that normalizes on a sample-by-sample unit, and is a way to complement for large gradients early in training in a transformer architecture.

I was also impressed with what this paper had to say about zero-shot pretrained learning.
Reading Transformers and revisiting the paper was a great experience as it gave me a better understanding of the background of the paper.

### Korean

최근에 "언어 모델은 비지도 다중 작업 학습자"라는 논문을 읽었습니다.
사실 전에 트랜스포머를 공부하기 전에 한 번 읽었던 논문입니다.
이 논문은 GPT-2에 대해 쓰여져 있습니다.

GPT-2와 GPT-1의 세 가지 차이점이 있습니다. 그래서 이 세 가지 특징을 리뷰하겠습니다.
첫째, GPT-2는 GPT-1보다 큽니다.
이것은 당연하지만, 이 특징은 이 논문에서 가장 큰 변화입니다.
둘째, GPT-2는 크고 훌륭한 데이터셋으로 훈련되었습니다.
Reddit에서 적어도 세 개의 추천을 받은 게시물을 스크래핑하여 큰 데이터셋을 구축했습니다.
셋째, GPT-2는 '레이어 정규화'를 사용했습니다.
레이어 정규화는 샘플 단위로 정규화하는 정규화 유형으로, 트랜스포머 아키텍처에서 초기 학습 단계에서 큰 기울기를 보완 위한 방법입니다.

GPT-2 논문에서는 zero-shot pretrained learning에 대해 시사한 점도 감명이 깊었습니다.
트랜스포머를 읽고 논문을 다시한번 더 본것은 이 논문의 배경을 더 잘 이해할 수 있어 좋은 경험이었습니다.

### GPT야 도와줘!!

I recently read the paper titled "Language Models are Unsupervised Multitask Learners" which, interestingly, pertains to GPT-2. This paper caught my attention as I had read it once before delving into the study of transformers.

Let's review three key differences between GPT-2 and GPT-1 highlighted in the paper. Firstly, GPT-2 is larger than GPT-1. While this might seem obvious, the paper emphasizes this feature as the most significant change. Secondly, GPT-2 was trained on a large and diverse dataset. The model was fine-tuned by scraping posts with at least three upvotes from Reddit, resulting in a substantial dataset. Thirdly, GPT-2 employs 'layer normalization.' This form of normalization, performed at the sample level, is a technique in the transformer architecture aimed at mitigating large gradients during the initial stages of training.

I found the paper's discussion on zero-shot pretrained learning particularly intriguing. Revisiting the paper after studying transformers provided me with a better understanding of its context, enhancing the overall learning experience.

### 새로운 표현

* pertains to : ~와 관련이 있다.
* caught my attention : 내 관심을 끌다.
* substantial : 상당한
* aimed at : ~의 대상
* intriguing : 흥미