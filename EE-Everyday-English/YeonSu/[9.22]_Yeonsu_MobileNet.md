# MobileNet

### Content

[MobileNet - Deep Convolutional Models: Case Studies | Coursera](https://www.coursera.org/learn/convolutional-neural-networks/lecture/B1kPZ/mobilenet)

### My draft

ML model은 시간이 지날수록 용량이 커지고 이를 모바일에서 사용하기에는 한계가 있습니다.  때문에 모바일에서 Deep 뉴럴 네트워크를 이용하여 추론하기 위해 모델을 경량화 하는 연구가 활발하게 진행되었습니다. 모바일넷은 그렇게 탄생한 네트워크입니다.

모바일넷에서 가장 중요한 경량화 방식은 Depthwise Separable Convolution 입니다. 이와 비교되는 것은 노말 컨볼루션입니다. 노말 컨볼루션의 컴퓨테이셔널 코스트는 다음과 같이 계산할 수 있습니다.  computational cost. = filter params x filter position x num of filters. 만약 6 x 6 x 3 의 이미지를 5개의 3 x 3 x 3의 filter로 컨볼루션 한다면 27 x 16 x 5 즉 2160의 연산이 필요합니다. Depthwise Separable Convolution은 3 x 3 x 3 filter 대신 3 x 3 x 1 filter를 사용하여 각각의 채널에 컨볼루션을 한 뒤  1 x 1 conv 를 이용하여 채널의 개수를 조절해줍니다. 이를 통해 앞부분에서는 9 x 16 x 3 = 432 의 연산만을 필요로 하며, 뒷 부분에서는 4 x 12 x 5 = 240 의 연산만을 필요로 합니다. 이 둘을 합치면 672의 연산만을 필요로 하고 이는 노말 컨볼루션에 비해 1/3 수준입니다. 이 경우에서는 1/3 의 결과가 나왔지만 일반적인 네트워크에서 depthwise separable convolution의 효율은 10배 정도가 된다고 mobilenet 의 저자들은 말합니다.

저번 글에서 작성한 inception module과 mobilenet에서 알수 있는 점은 1x1 conv가 연산량을 획기적으로 줄여준다는 점입니다. 이에 굉장히 큰 흥미가 생겼고 요즘 공부중인 low rank와 연관지어 공부할 계획입니다.

### Translate

ML models grow in size over time, and there are limitations to using them on mobile, so there has been a lot of research into lightweighting models for inference using deep neural networks on mobile. This is how MobileNet was born.

The most important lightweighting method in MobileNet is Depthwise Separable Convolution. It is compared to normal convolution. The computational cost of normal convolution can be calculated as follows: computational cost = filter params x filter position x num of filters. If you want to convolve a 6 x 6 x 3 image with five 3 x 3 x 3 filters, you need 27 x 16 x 5, 2160 operations. Instead of 3 x 3 x 3 filters, Depthwise Separable Convolution uses 3 x 3 x 1 filters to convolve each channel, and then uses 1 x 1 conv to adjust the number of channels. This results in only 9 x 16 x 3 = 432 operations in the front part, and 4 x 12 x 5 = 240 operations in the back part. Combined, they only require 672 operations, which is a third of the computation of the normal convolution. In this case, the result is 1/3, but the authors of MobileNet was like that the efficiency of depthwise separable convolution in general networks is about 10 times.

What we can see from the inception module and mobilenet is that 1x1 conv can dramatically reduce the amount of computation. I am very interested in this and plan to study it in connection with low rank, which I am currently studying.