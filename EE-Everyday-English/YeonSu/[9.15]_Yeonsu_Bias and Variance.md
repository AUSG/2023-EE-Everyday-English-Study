### Content URL : https://www.coursera.org/learn/deep-neural-network/lecture/ZBkx4/basic-recipe-for-machine-learning

### New Expressions

On the opposite end : 반대로</br>
평소에 자주쓰는 표현이지만 In contrast, Conversely, 와 같은 표현을 많이 썼었는데 새롭게 알게 되었습니다. 앞으로 종종 이용해볼게요!

## My draft
Watching this video, there's often more nuance to bias and variance than i expect. For examples, When i heard bias and variance, i used to think of the bias-variance trade off. But the content says there's less of a trade-off in deep learning error. So, I thought i wasn't keeping up with the trends.

In this post, i will write about the basic concepts of bias and variance not trade-offs because I think organizing that concepts is important. i really want to keeping up with the trends!

Bias represents the average of the difference between the predictions obtained by the model and the actual correct answer. If the error is big, we says that model has high bias.

On the oppsite end, Variance is a quantitive concept of how much a prediction can change for different data sets. It means if the model has high variance, the model is overfitted!

Best model has low bias and low variance. And worst model has high bias and high variance. Analyzing bias and variance is important because it allows us to systematically improve our model's algorithms.

## DeepL 번역
이 동영상을 보면서 편향성과 분산에는 제가 생각했던 것보다 더 많은 뉘앙스가 있다는 것을 알게 되었습니다. 예를 들어, 저는 편향성과 분산이라고 하면 편향성과 분산이 상충한다고 생각했었습니다. 하지만 내용에는 딥러닝 오류에는 트레이드 오프가 적다고 나와 있습니다. 그래서 제가 트렌드를 따라가지 못하고 있다는 생각이 들었습니다.

이번 포스팅에서는 트레이드 오프가 아닌 편향과 분산에 대한 기본 개념을 정리하는 것이 중요하다고 생각해서, 트렌드를 따라잡고 싶은 마음에 편향과 분산에 대한 기본 개념에 대해 써보려고 합니다!

편향성은 모델에서 얻은 예측과 실제 정답 사이의 차이의 평균을 나타냅니다. 오차가 크면 모델의 편향성이 높다고 말합니다.

반대로 분산은 다른 데이터 세트에 대해 예측이 얼마나 많이 변할 수 있는지를 나타내는 정량적 개념입니다. 즉, 모델의 분산이 크면 모델이 과적합한 것입니다!

최상의 모델은 편향이 낮고 분산이 낮습니다. 그리고 최악의 모델은 편향이 높고 분산이 높습니다. 편향과 분산을 분석하는 것은 모델의 알고리즘을 체계적으로 개선할 수 있기 때문에 중요합니다.

## Chat-GPT 도와줘

Watching this video, I've realized that there's often more nuance to bias and variance than I initially expected. For example, when I used to hear "bias and variance," I used to think of the bias-variance trade-off. However, the content suggests that there's less of a trade-off in deep learning errors. This made me feel like I might not be keeping up with the latest trends.

In this post, I want to discuss the fundamental concepts of bias and variance, rather than focusing on trade-offs, as I believe organizing these concepts is crucial for staying updated with the latest developments.

Bias represents the average difference between the model's predictions and the actual correct answers. If this error is substantial, we say that the model has high bias.

On the other hand, variance is a quantitative measure of how much predictions can vary across different datasets. In other words, if a model has high variance, it tends to overfit.

The best model exhibits low bias and low variance, while the worst model has high bias and high variance. Analyzing bias and variance is essential because it enables us to systematically improve our model's algorithms.
