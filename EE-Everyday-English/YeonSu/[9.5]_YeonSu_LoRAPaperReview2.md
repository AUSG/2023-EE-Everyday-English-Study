### My draft

Today, I read Problem Statement of LoRA paper.

The authors says existed problem of llm’s fine-tuning and their efficient fine-tuning solution named LoRA.

I was excited in this solution, so I want to introduce this solution for you!

In this paper, existed llm’s find-tuning has problem of memory available.

If user want N downstream tasks using llm that sized M, It required N * M memories.

But the llm’s size is too large to store all models in general environment.

They adopt a more parameter-efficient approach, where the task-specific parameter by adapting row rank dimension decompoisition.

For Example, n*n matrix can decomposed to n*m * m * n. 

This innovative idea can train using small resources by frozen existed n * n parameters.

This solution is good but I worried inference latency by additional layer. 

I think the solution about I worried will be proposed in this paper’s Algorithm section.

Thank you.!!

### DeepL translate my sentences in korean. i'm satisfied

오늘 LoRA 논문의 문제점을 읽었습니다.

저자들은 llm의 미세 조정에 대한 문제점이 존재했고, 이에 대한 효율적인 미세 조정 솔루션으로 LoRA라는 솔루션을 개발했다고 합니다.

저는 이 솔루션에 흥미를 느꼈기 때문에 여러분께 소개해드리려고 합니다!

이 논문에서는 기존 llm의 파인튜닝은 메모리 가용성 문제가 있었습니다.

사용자가 M 크기의 llm을 사용하여 N개의 다운스트림 작업을 수행하려면 N * M 메모리가 필요했습니다.

하지만 일반적인 환경에서 모든 모델을 저장하기에는 llm의 크기가 너무 큽니다.

행 순위 차원 압축 해제를 통해 작업별 매개변수를 조정하는 보다 매개변수 효율적인 접근 방식을 채택했습니다.

예를 들어, n*n 행렬은 n*m * m * n으로 분해할 수 있습니다. 

이 혁신적인 아이디어는 기존의 n * n 파라미터를 동결하여 적은 리소스를 사용하여 학습할 수 있습니다.

이 솔루션도 좋지만 추가 레이어에 의한 추론 지연시간이 걱정되었습니다. 

걱정했던 부분에 대한 해결책은 본 논문의 알고리즘 섹션에서 제시될 것 같습니다.

감사합니다!!!

### New sentences that revised by ChatGPT

Today, I read the Problem Statement of the LoRA paper.

The authors discuss the existing problems with fine-tuning Large Language Models (LLMs) and present their efficient fine-tuning solution called LoRA.

I was excited about this solution, so I would like to introduce it to you!

In this paper, they address the problem of limited memory availability when fine-tuning existing LLMs. When a user wants to perform N downstream tasks using an LLM of size M, it typically requires N * M memory. However, the size of LLMs is often too large to store all models in a typical environment.

To tackle this issue, they adopt a more parameter-efficient approach by adapting row-rank dimension decomposition. For example, an n * n matrix can be decomposed into an n * m * m * n structure. This innovative idea allows training using fewer resources while freezing the existing n * n parameters.

Although this solution is promising, I have concerns about potential inference latency caused by the additional layer. I believe the paper may address my concerns in the Algorithm section.

Thank you!

### 챗지피티의 칭찬...

Your English is improving, and your message is clear. I made some minor revisions to improve the clarity and flow of the text. Keep up the good work! If you have any further questions or need more assistance, feel free to ask.

### Expression I didn't know

address ~다루다., 해결하다.

ex)

I will address the issue with the broken printer as soon as I get to the office.

사무실에 도착하는 대로 고장난 프린터 문제를 해결하겠습니다.