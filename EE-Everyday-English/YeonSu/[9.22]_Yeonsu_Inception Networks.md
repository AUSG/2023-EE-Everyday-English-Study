# Inception Network

### Content

[Inception Network - Deep Convolutional Models: Case Studies | Coursera](https://www.coursera.org/learn/convolutional-neural-networks/lecture/piR0x/inception-network)

### My Draft

예전에 googleNet 논문을 읽다가 포기한 적이 있습니다. googleNet은 Inception Module로 이루어져 있는데, 당시에는 Inception module이 뭔지 몰랐기 때문입니다. 하지만 이 동영상을 보면서 Inception module의 개념과 최적화 한 방식에 대해서 알 수 있었습니다. 이 글에서 Inception module에 대한 설명과 영상 리뷰를 작성해보겠습니다.

Inception module은 filter의 종류를 하나만 고르기 싫다는 아이디어에서 고안되었습니다. convolution network에서 filter의 크기를 고르는 것은 중요한 작업일 수 있습니다. 왜냐하면 filter의 크기에 따라서 feature map의 크기가 달라지기 때문입니다. 하지만 한번 학습할 때마다 filter를 교체하여 실험하는 것은 비효율적일 수 있습니다. Inception module은 한 layer에 1, 3, 5 크기의 필터와 심지어는 pool layer를 한번에 넣었습니다. 이를 이용해서 다양한 filter로 부터 추출한 정보들을 한 layer에서 이용할 수 있습니다. 하지만 inception module에 문제는 3, 5 크기의 필터에서 발생합니다. 3, 5 크기의 필터는 이전 layer에서 채널의 개수가 많으면 trainable parameters의 개수가 너무 커진다는 단점이 있습니다. inception module은 이 문제를 1x1 conv layer를 이용해서 해결했습니다. 1 x 1 conv layer는 채널의 개수를 조절할 수 있다는 장점이 있기 때문에 채널의 개수를 줄였다가 3, 5 크기의 filter를 사용할 때 다시 원하는 크기의 채널의 개수로 늘릴 수 있습니다. 이러면 feature map의 모양은 작아졌다가 다시 커지는 모양이 되는데 이것을 bottleneck이라고 합니다.

이 영상을 보면서 inception module과 1x1 conv layer의 역할을 확실하게 알 수 있었습니다. coursera 강의들은 제가 지금까지 놓쳤던 dl 지식들을 다시 정리할 수 있어서 정말 좋습니다.

### Translate

I once gave up on reading the googleNet paper because I didn't know what an Inception module was. googleNet is made up of Inception modules, but after watching this video, I was able to understand the concept of Inception modules and how they are optimized. In this article, I will explain and review the Inception module.

The idea behind the Inception module is that you don't want to choose just one type of filter. Choosing the size of the filter in a convolutional network can be an important task, because the size of the filter determines the size of the feature map. However, it can be inefficient to experiment by changing the filter every time you train. The Inception module puts filters of sizes 1, 3, 5, and even a pool layer in one layer, so that the information extracted from different filters can be used in one layer. However, the problem with the inception module is with the 3 and 5 filters. The problem with 3 and 5 filters is that if the number of channels in the previous layer is large, the number of trainable parameters becomes too large. The inception module solves this problem by using a 1x1 conv layer, which has the advantage that the number of channels is adjustable, so you can reduce the number of channels and then increase it back to the desired number of channels when using 3 and 5-sized filters. In this case, the shape of the feature map becomes smaller and then larger, which is called a bottleneck.

Watching this video, I was able to clearly see the role of the inception module and the 1x1 conv layer. Coursera courses are great for refreshing my DL knowledge that I've been missing.